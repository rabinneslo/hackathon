{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (19,) (12,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-45a96cba5f17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;31m# db = davies-bouldin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;31m# forest_run(dimensionality, patterns_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m     \u001b[0mforest_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdimensionality\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_down\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_up\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m     \u001b[1;31m# forest_run(dimensionality, patterns_data, metric='si')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-45a96cba5f17>\u001b[0m in \u001b[0;36mforest_run\u001b[1;34m(dimensions, patterns, pattern_labels, metric, k_up, k_down, simulations, iterations)\u001b[0m\n\u001b[0;32m    500\u001b[0m                 \u001b[1;31m# create a clustering solution and apply k-means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m                 \u001b[0mclustering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m                 \u001b[0mclustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_means_clustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m                 \u001b[1;31m# used to compute quality of the solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[0mquality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClusteringQuality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-45a96cba5f17>\u001b[0m in \u001b[0;36mk_means_clustering\u001b[1;34m(self, n, s)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-45a96cba5f17>\u001b[0m in \u001b[0;36massign_patterns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;31m# calculate the distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfractional_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[1;31m# assign the pattern to a cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_pattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-45a96cba5f17>\u001b[0m in \u001b[0;36mfractional_distance\u001b[1;34m(self, p_vec, q_vec, fraction)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemoization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                 \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_vec\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mq_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[0mdiff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (19,) (12,) "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import cProfile\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "memoization = {}\n",
    "\n",
    "\n",
    "class Clustering:\n",
    "    \"\"\"\n",
    "    An instance of the Clustering is a solution i.e. a particular partitioning of the (heterogeneous) data set into\n",
    "    homogeneous subsets. For Centroid based clustering algorithms this involves looking at each pattern and assigning\n",
    "    it to it's nearest centroid. This is done by calculating the distance between each pattern and every centroid and\n",
    "    selecting the one with the smallest distance. Here we use are using fractional distance with the default parameters.\n",
    "\n",
    "    :param d: dimensionality of the input patterns\n",
    "    :param k: the pre-specified number of clusters & centroids\n",
    "    :param z: the patterns in the data set\n",
    "    :param min: the minimum distance (required to prevent division by zero)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, k, z, min):\n",
    "        # print(\"Initializing solution ...\")\n",
    "        \"\"\"\n",
    "        Initializes a Clustering object with the specified parameters\n",
    "        :param d: dimensionality of the input patterns\n",
    "        :param k: the pre-specified number of clusters & centroids\n",
    "        :param z: the patterns in the data set\n",
    "        :param min: the minimum distance (required to prevent division by zero)\n",
    "        \"\"\"\n",
    "        self.dimensionality = d\n",
    "        self.num_clusters = k\n",
    "        self.patterns = z\n",
    "        self.solution = []\n",
    "        for i in range(len(z)):\n",
    "            self.solution.append(0)\n",
    "        self.centroids = np.random.rand(k, d)\n",
    "        self.e = min\n",
    "\n",
    "    def re_init(self):\n",
    "        \"\"\"\n",
    "        A method for reinitializing the solution\n",
    "        \"\"\"\n",
    "        self.centroids = None\n",
    "        self.centroids = np.random.rand(self.num_clusters, self.dimensionality)\n",
    "\n",
    "    def assign_patterns(self):\n",
    "        \"\"\"\n",
    "        This method iterates over all patterns and calculates the distances between that pattern and every centroid.\n",
    "        These value are stored in [distances]. The assign_pattern method is then used to find the centroid with the\n",
    "        smallest distance and update the 'label' i.e. centroid which the pattern is associated.\n",
    "        \"\"\"\n",
    "        s = Similarity(self.e)\n",
    "        # for each pattern\n",
    "        for i in range(len(self.patterns)):\n",
    "            # for each centroid\n",
    "            distances = []\n",
    "            for j in range(self.num_clusters):\n",
    "                # calculate the distances\n",
    "                distances.append(s.fractional_distance(self.centroids[j], self.patterns[i]))\n",
    "            # assign the pattern to a cluster\n",
    "            self.assign_pattern(distances, i)\n",
    "\n",
    "    def assign_pattern(self, distances, index):\n",
    "        \"\"\"\n",
    "        This method updates the label i.e. centroid index \\in (0, k-1) to which pattern z(index) belongs\n",
    "        :param distances: distances to each centroid\n",
    "        :param index: the index of the pattern we are assigning in z\n",
    "        \"\"\"\n",
    "        self.solution[index] = 0\n",
    "        smallest = distances[self.solution[index]]\n",
    "        for i in range(len(distances)):\n",
    "            if distances[i] < smallest:\n",
    "                smallest = distances[i]\n",
    "                self.solution[index] = i\n",
    "\n",
    "    def update_centroids(self, s=1.0):\n",
    "        \"\"\"\n",
    "        This method implements the mean-shift heuristic used by the K-means clustering algorithm. This heuristic\n",
    "        updates the value of each centroid with the average value at each dimension of the patterns assigned to it.\n",
    "        :param s: this is the scaling factor i.e. how much we want to diminish the movement of the centroids by\n",
    "        \"\"\"\n",
    "        # Step 1 - initialize a variable to store the sum at each dimension of the patterns assigned to each centroid\n",
    "        centroids_sum = []\n",
    "        for i in range(self.num_clusters):\n",
    "            centroids_sum.append([])\n",
    "            for j in range(self.dimensionality):\n",
    "                centroids_sum[i].append(0.0)\n",
    "        # Step 2 - initialize a variable to store the count of patterns assigned to each centroid\n",
    "        centroids_count = []\n",
    "        for i in range(self.num_clusters):\n",
    "            centroids_count.append(0.0)\n",
    "        # Step 3 - Update the value of centroids_sum and centroids_count for step 4\n",
    "        for i in range(len(self.solution)):\n",
    "            for j in range(self.dimensionality):\n",
    "                centroids_sum[self.solution[i]][j] += self.patterns[i][j]\n",
    "            centroids_count[self.solution[i]] += 1\n",
    "        # Step 4 - compute the averages (total / count) for each dimension for each centroid\n",
    "        centroids_average = []\n",
    "        for i in range(self.num_clusters):\n",
    "            centroids_average.append([])\n",
    "            for j in range(self.dimensionality):\n",
    "                if centroids_count[i] > 0:\n",
    "                    centroids_average[i].append(centroids_sum[i][j] / max(1.0, centroids_count[i]))\n",
    "                else:\n",
    "                    centroids_average[i].append(random.random())\n",
    "        # Step 5 - set  each dimension of each centroid to the average of it's clusters values at that dimension\n",
    "        for i in range(self.num_clusters):\n",
    "            if s == 1.0:\n",
    "                self.centroids[i] = None\n",
    "                self.centroids[i] = centroids_average[i]\n",
    "            else:\n",
    "                for j in range(len(self.centroids[i])):\n",
    "                    self.centroids[i][j] += (centroids_average[i][j] - self.centroids[i][j]) * s\n",
    "\n",
    "    def k_means_clustering(self, n, s=1.0):\n",
    "        \"\"\"\n",
    "        This method performs the K-means clustering algorithm on the data for n iterations. This involves updating the\n",
    "        centroids using the mean-shift heuristic n-times and reassigning the patterns to their closest centroids.\n",
    "        :param n: number of iterations to complete\n",
    "        :param s: the scaling factor to use when updating the centroids\n",
    "        pick on which has a better solution (according to some measure of cluster quality)\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(n):\n",
    "            self.assign_patterns()\n",
    "            self.update_centroids(s)\n",
    "\n",
    "    def print_solution(self, labels):\n",
    "        \"\"\"\n",
    "        Prints out the clustering i.e. which patterns are assigned to which centroids. This can be cross-referenced\n",
    "        with the label on each pattern to determine which countries are clustered together in space.\n",
    "        :param labels: pattern labels\n",
    "        \"\"\"\n",
    "        for i in range(len(self.solution)):\n",
    "            print(labels[i], \",\", self.solution[i])\n",
    "\n",
    "\n",
    "class ClusteringQuality:\n",
    "    \"\"\"\n",
    "    Instances of this class implement the two measures of clustering quality discussed in the article, namely the davies\n",
    "    bouldin index and the silhouette index. It also implements a number of useful helper methods.\n",
    "    :param solution: the clustering solution of type Clustering\n",
    "    :param minimum: the minimum distance allowable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, solution, minimum):\n",
    "        \"\"\"\n",
    "        Initializes a ClusteringQuality object with a given Clustering solution and a minimum distance\n",
    "        :param solution: this is an object of type Clustering\n",
    "        :param minimum: this is the minimum distance allowed between two points\n",
    "        \"\"\"\n",
    "        assert isinstance(solution, Clustering)\n",
    "        self.solution = solution\n",
    "        self.e = minimum\n",
    "\n",
    "    def cluster_totals(self):\n",
    "        \"\"\"\n",
    "        This method calculates the total distance from every centroid to every pattern assigned to it. It also records\n",
    "        the number of patterns in each cluster which are used to compute average distances in cluster_averages()\n",
    "        :return: a two dimensional list of [total cluster distance, total patterns in cluster] for each centroid\n",
    "        \"\"\"\n",
    "        s = Similarity(self.e)\n",
    "        # create array (will be 2d) to store total internal cluster distances and cluster counts for each centroid\n",
    "        cluster_distances_counts = []\n",
    "        for i in range(self.solution.num_clusters):\n",
    "            ith_cluster_count = 0.0\n",
    "            ith_cluster_distance = 0.0\n",
    "            for z in range(len(self.solution.solution)):\n",
    "                # update the count and the total distance for the centroid z[i] belongs to (whichever one that is)\n",
    "                if self.solution.solution[z] == i:\n",
    "                    ith_cluster_count += 1\n",
    "                    ith_cluster_distance += s.fractional_distance(self.solution.patterns[z], self.solution.centroids[i])\n",
    "            # add the result to the 2d list\n",
    "            cluster_distances_counts.append([ith_cluster_distance, max(ith_cluster_count, 1.0)])\n",
    "        return np.array(cluster_distances_counts)\n",
    "\n",
    "    def cluster_averages(self):\n",
    "        \"\"\"\n",
    "        Receives output from cluster_totals() and computes the average distance per centroid\n",
    "        :return: average distance from each centroid to the patterns assigned to it\n",
    "        \"\"\"\n",
    "        # create list to store averages in\n",
    "        cluster_averages = []\n",
    "        # get the total internal cluster distances plus the counts for each centroid / cluster\n",
    "        cluster_distances_counts = self.cluster_totals()\n",
    "        for i in range(len(cluster_distances_counts)):\n",
    "            # calculate the averages and add it to the list\n",
    "            cluster_averages.append(cluster_distances_counts[i][0] / cluster_distances_counts[i][1])\n",
    "        return np.array(cluster_averages)\n",
    "\n",
    "    def davies_bouldin(self):\n",
    "        \"\"\"\n",
    "        This method computes the davies-bouldin (db) of a given clustering.\n",
    "        :return: the davies bouldin value of the clustering\n",
    "        \"\"\"\n",
    "        # get the average internal cluster distances\n",
    "        cluster_averages = self.cluster_averages()\n",
    "        # create variable for db\n",
    "        davies_bouldin = 0.0\n",
    "        s = Similarity(self.e)\n",
    "        # for each cluster / centroid i\n",
    "        for i in range(self.solution.num_clusters):\n",
    "            # for each cluster / centroid j\n",
    "            for j in range(self.solution.num_clusters):\n",
    "                # when i and j are not the same cluster / centroid\n",
    "                if j != i:\n",
    "                    # calculate the distance between the two centroids of i and j\n",
    "                    d_ij = s.fractional_distance(self.solution.centroids[i], self.solution.centroids[j])\n",
    "                    # update the variable to equal to sum of internal cluster distances of clusters i and j divided by\n",
    "                    # the previously computer value i.e. the distance between centroid i and centroid j\n",
    "                    d_ij = (cluster_averages[i] + cluster_averages[j]) / d_ij\n",
    "                    # update db is this is larger than any db seen before\n",
    "                    davies_bouldin = max(d_ij, davies_bouldin)\n",
    "        return davies_bouldin\n",
    "\n",
    "    def silhouette_index(self, index):\n",
    "        \"\"\"\n",
    "        This method computes the silhouette index (si) for any given pattern between -1 and 1\n",
    "        :param index: the pattern we are looking at now\n",
    "        :return: the silhouette index for that pattern\n",
    "        \"\"\"\n",
    "        # store the total distance to each cluster\n",
    "        silhouette_totals = []\n",
    "        # store the number of patterns in each cluster\n",
    "        silhouette_counts = []\n",
    "        # initialize the variables\n",
    "        for i in range(self.solution.num_clusters):\n",
    "            silhouette_totals.append(0.0)\n",
    "            silhouette_counts.append(0.0)\n",
    "        s = Similarity(self.e)\n",
    "        for i in range(len(self.solution.patterns)):\n",
    "            # for every pattern other than the one we are calculating now\n",
    "            if i != index:\n",
    "                # get the distance between pattern[index] and that pattern\n",
    "                distance = s.fractional_distance(self.solution.patterns[i], self.solution.patterns[index])\n",
    "                # add that distance to the silhouette totals for the correct cluster\n",
    "                silhouette_totals[self.solution.solution[i]] += distance\n",
    "                # update the number of patterns in that cluster\n",
    "                silhouette_counts[self.solution.solution[i]] += 1\n",
    "        # setup variable to find the cluster (not equal to the pattern[index]'s cluster) with the smallest distance\n",
    "        smallest_silhouette = silhouette_totals[0] / max(1.0, silhouette_counts[0])\n",
    "        for i in range(len(silhouette_totals)):\n",
    "            # calculate the average distance of each pattern in that cluster from pattern[index]\n",
    "            silhouette = silhouette_totals[i] / max(1.0, silhouette_counts[i])\n",
    "            # if the average distance is lower and it isn't pattern[index] cluster update the value\n",
    "            if silhouette < smallest_silhouette and i != self.solution.solution[index]:\n",
    "                smallest_silhouette = silhouette\n",
    "        # calculate the internal cluster distances for pattern[index]\n",
    "        index_cluster = self.solution.solution[index]\n",
    "        index_silhouette = self.e + silhouette_totals[index_cluster] / max(1.0, silhouette_counts[index_cluster])\n",
    "        # return the ratio between the smallest distance from pattern[index] to another cluster's patterns and\n",
    "        # the patterns belong to the same cluster as pattern[index]\n",
    "        return (smallest_silhouette - index_silhouette) / max(smallest_silhouette, index_silhouette)\n",
    "\n",
    "    def silhouette_index_zero_one(self, index):\n",
    "        \"\"\"\n",
    "        Returns the silhouette index between 0 and 1 and makes it a minimization objective (easier)\n",
    "        :param index: the pattern we are looking at now\n",
    "        :return: the silhouette index for that pattern\n",
    "        \"\"\"\n",
    "        return 1 - ((1 + self.silhouette_index(index)) / 2.0)\n",
    "\n",
    "    def average_silhouette_index(self, scaled_zero_one=True):\n",
    "        \"\"\"\n",
    "        This method computes the average silhouette index value every pattern in the data set.\n",
    "        :param scaled_zero_one: allows you to scale the result between 0 and 1 and reverse the order\n",
    "        :return: the silhouette index of the given clustering\n",
    "        \"\"\"\n",
    "        silhouette_sum = 0.0\n",
    "        for i in range(len(self.solution.patterns)):\n",
    "            if scaled_zero_one:\n",
    "                silhouette_sum += self.silhouette_index_zero_one(i)\n",
    "            else:\n",
    "                silhouette_sum += self.silhouette_index(i)\n",
    "        return silhouette_sum / len(self.solution.patterns)\n",
    "\n",
    "    def quantization_error(self):\n",
    "        \"\"\"\n",
    "        This method calculates the quantization error of the given clustering\n",
    "        :return: the quantization error\n",
    "        \"\"\"\n",
    "        total_distance = 0.0\n",
    "        s = Similarity(self.e)\n",
    "        for i in range(len(self.solution.patterns)):\n",
    "            total_distance += math.pow(s.fractional_distance(self.solution.patterns[i],\n",
    "                                                             self.solution.centroids[self.solution.solution[i]]), 2.0)\n",
    "        return total_distance / len(self.solution.patterns)\n",
    "\n",
    "\n",
    "class Similarity:\n",
    "    \"\"\"\n",
    "    This class contains instances of similarity / distance metrics. These are used in centroid based clustering\n",
    "    algorithms to identify similar patterns and put them into the same homogeneous sub sets\n",
    "    :param minimum: the minimum distance between two patterns (so you don't divide by 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, minimum):\n",
    "        self.e = minimum\n",
    "        self.vector_operators = VectorOperations()\n",
    "\n",
    "    def manhattan_distance(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the manhattan distance metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the manhattan distance between vector one and two\n",
    "        \"\"\"\n",
    "        return max(np.sum(np.fabs(p_vec - q_vec)), self.e)\n",
    "\n",
    "    def square_euclidean_distance(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the squared euclidean distance metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the squared euclidean distance between vector one and two\n",
    "        \"\"\"\n",
    "        diff = p_vec - q_vec\n",
    "        return max(np.sum(diff ** 2), self.e)\n",
    "\n",
    "    def euclidean_distance(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the euclidean distance metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the euclidean distance between vector one and two\n",
    "        \"\"\"\n",
    "        return max(math.sqrt(self.square_euclidean_distance(p_vec, q_vec)), self.e)\n",
    "\n",
    "    def half_square_euclidean_distance(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the half squared euclidean distance metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the half squared euclidean distance between vector one and two\n",
    "        \"\"\"\n",
    "        return max(0.5 * self.square_euclidean_distance(p_vec, q_vec), self.e)\n",
    "\n",
    "    def cosine_similarity(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the cosine similarity metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the cosine similarity between vector one and two\n",
    "        \"\"\"\n",
    "        pq = self.vector_operators.product(p_vec, q_vec)\n",
    "        p_norm = self.vector_operators.norm(p_vec)\n",
    "        q_norm = self.vector_operators.norm(q_vec)\n",
    "        return max(pq / (p_norm * q_norm), self.e)\n",
    "\n",
    "    def tanimoto_coefficient(self, p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method implements the cosine tanimoto coefficient metric\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the tanimoto coefficient between vector one and two\n",
    "        \"\"\"\n",
    "        pq = self.vector_operators.product(p_vec, q_vec)\n",
    "        p_square = self.vector_operators.square(p_vec)\n",
    "        q_square = self.vector_operators.square(q_vec)\n",
    "        return max(pq / (p_square + q_square - pq), self.e)\n",
    "\n",
    "    def fractional_distance(self, p_vec, q_vec, fraction=2.0):\n",
    "        \"\"\"\n",
    "        This method implements the fractional distance metric. I have implemented memoization for this method to reduce\n",
    "        the number of function calls required. The net effect is that the algorithm runs 400% faster. A similar approach\n",
    "        can be used with any of the above distance metrics as well.\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :param fraction: the fractional distance value (power)\n",
    "        :return: the fractional distance between vector one and two\n",
    "        \"\"\"\n",
    "        # memoization is used to reduce unnecessary calculations ... makes a BIG difference\n",
    "        memoize = True\n",
    "        if memoize:\n",
    "            key = self.get_key(p_vec, q_vec)\n",
    "            x = memoization.get(key)\n",
    "            if x is None:\n",
    "                diff = p_vec - q_vec\n",
    "                diff_fraction = diff ** fraction\n",
    "                return max(math.pow(np.sum(diff_fraction), 1 / fraction), self.e)\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            diff = p_vec - q_vec\n",
    "            diff_fraction = diff ** fraction\n",
    "            return max(math.pow(np.sum(diff_fraction), 1 / fraction), self.e)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key(p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method returns a unique hash value for two vectors. The hash value is equal to the concatenated string of\n",
    "        the hash value for vector one and vector two. E.g. is hash(p_vec) = 1234 and hash(q_vec) = 5678 then get_key(\n",
    "        p_vec, q_vec) = 12345678. Memoization improved the speed of this algorithm 400%.\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: a unique hash\n",
    "        \"\"\"\n",
    "        # return str(hash(tuple(p_vec))) + str(hash(tuple(q_vec)))\n",
    "        return str(hashlib.sha1(p_vec)) + str(hashlib.sha1(q_vec))\n",
    "\n",
    "\n",
    "class VectorOperations():\n",
    "    \"\"\"\n",
    "    This class contains useful implementations of methods which can be performed on vectors\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def product(p_vec, q_vec):\n",
    "        \"\"\"\n",
    "        This method returns the product of two lists / vectors\n",
    "        :param p_vec: vector one\n",
    "        :param q_vec: vector two\n",
    "        :return: the product of p_vec and q_vec\n",
    "        \"\"\"\n",
    "        return p_vec * q_vec\n",
    "\n",
    "    @staticmethod\n",
    "    def square(p_vec):\n",
    "        \"\"\"\n",
    "        This method returns the square of a vector\n",
    "        :param p_vec: the vector to be squared\n",
    "        :return: the squared value of the vector\n",
    "        \"\"\"\n",
    "        return p_vec ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(p_vec):\n",
    "        \"\"\"\n",
    "        This method returns the norm value of a vector\n",
    "        :param p_vec: the vector to be normed\n",
    "        :return: the norm value of the vector\n",
    "        \"\"\"\n",
    "        return np.sqrt(p_vec)\n",
    "\n",
    "\n",
    "class Data():\n",
    "    \"\"\"\n",
    "    A class for downloading data from a CSV file\n",
    "    :param file_name: the file name\n",
    "    :return: the data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        return\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        This method opens the file and reads it in\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        loaded_patterns = []\n",
    "        file = open(self.file_name)\n",
    "        row_number = 0\n",
    "        labels = []\n",
    "        for row in csv.reader(file):\n",
    "            if row_number != 0:\n",
    "                floats = []\n",
    "                for j in range(len(row)):\n",
    "                    if j != 0:\n",
    "                        floats.append(parseFloat(row[j]))\n",
    "                    else:\n",
    "                        labels.append(row[j])\n",
    "                loaded_patterns.append(floats)\n",
    "            row_number += 1\n",
    "        return np.array(loaded_patterns), labels\n",
    "def parseFloat(s):\n",
    "    result = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n",
    "    if len(result)>0:\n",
    "        return float(result[0])\n",
    "    return None\n",
    "\n",
    "def forest_run(dimensions, patterns, pattern_labels, metric='qe', k_up=20, k_down=2, simulations=55, iterations=50):\n",
    "    \"\"\"\n",
    "    A method for watching Forest Gump run\n",
    "    :param dimensions: the dimensionality of the data\n",
    "    :param patterns: the data itself\n",
    "    :param metric: the quality metric\n",
    "    :param k_up: the maximum number of clusters\n",
    "    :param k_down: the minimum number of clusters\n",
    "    :param simulations: the number of simulations for each k\n",
    "    :param iterations: the number of iterations for each k-means pass\n",
    "    \"\"\"\n",
    "    # variable to store the best result\n",
    "    best_clustering = None\n",
    "    # the quality of that result\n",
    "    best_quality = 1000.00\n",
    "    # write results out to file while simulating\n",
    "    file_out = 'result_cluster' + '_' + metric + '.csv'\n",
    "    with open(file_out, 'w', newline='') as f:\n",
    "        # different k values to test on\n",
    "        for i in range(k_down, k_up):\n",
    "            num_clusters = i\n",
    "            # number of retries / simulations\n",
    "            for j in range(simulations):\n",
    "                # create a clustering solution and apply k-means\n",
    "                clustering = Clustering(dimensions, num_clusters, patterns, 0.0001)\n",
    "                clustering.k_means_clustering(iterations)\n",
    "                # used to compute quality of the solution\n",
    "                quality = ClusteringQuality(clustering, 0.0001)\n",
    "                this_quality = 0.0\n",
    "                if metric == 'qe':\n",
    "                    this_quality = quality.quantization_error()\n",
    "                if metric == 'si':\n",
    "                    this_quality = quality.average_silhouette_index()\n",
    "                if metric == 'db':\n",
    "                    this_quality = quality.davies_bouldin()\n",
    "                # update the best clustering\n",
    "                if this_quality < best_quality:\n",
    "                    best_quality = this_quality\n",
    "                    best_clustering = clustering\n",
    "                    print(\"Updated best clustering\")\n",
    "                # write result to the file\n",
    "                result = [num_clusters, this_quality]\n",
    "                for x in result:\n",
    "                    f.write(str(x))\n",
    "                    f.write(\",\")\n",
    "                f.write(\"\\n\")\n",
    "                f.flush()\n",
    "                print(j, result)\n",
    "        # print the actual clustering out to console\n",
    "        best_clustering.print_solution(pattern_labels)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # cProfile.run('forest_run()')\n",
    "    # set the number of dimensions in the data\n",
    "    dimensionality = 12\n",
    "    # load the data into an object\n",
    "    data = Data(\"cluster_data.csv\")\n",
    "    # get the patterns from the object (list of lists)\n",
    "    pattern_labels = []\n",
    "    patterns_data, pattern_labels = data.load_data()\n",
    "    # specify the metric\n",
    "    # qe = quantization error\n",
    "    # si = silhouette index\n",
    "    # db = davies-bouldin\n",
    "    # forest_run(dimensionality, patterns_data)\n",
    "    forest_run(dimensionality, patterns_data, pattern_labels, simulations=1000, k_down=6, k_up=9)\n",
    "    # forest_run(dimensionality, patterns_data, metric='si')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
